<!doctype html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    
      blog &middot; parker higgins dot net
    
  </title>

  <link rel="stylesheet" href="/dotnet/styles.css">
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/dotnet/assets/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/dotnet/assets/favicon.ico">
  <link rel="alternate" type="application/atom+xml" title="parker higgins dot net" href="/dotnet/feed.xml">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="blog" />
<meta name="author" content="Parker Higgins" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://thisisparker.github.io/dotnet/blog/page3/" />
<meta property="og:url" content="https://thisisparker.github.io/dotnet/blog/page3/" />
<meta property="og:site_name" content="parker higgins dot net" />
<meta property="og:type" content="website" />
<link rel="prev" href="https://thisisparker.github.io/dotnet/blog/page2/" />
<link rel="next" href="https://thisisparker.github.io/dotnet/blog/page4/" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="blog" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Parker Higgins","url":"https://twitter.com/xor"},"headline":"blog","url":"https://thisisparker.github.io/dotnet/blog/page3/"}</script>
<!-- End Jekyll SEO tag -->

</head>


  <body>

    <div class="container content">
      <header class="masthead">

        <ul id="navbar">
  <li><a href="/dotnet//">about</a></li>
  <li><a href="/dotnet//projects">projects</a></li>
  <li><a href="/dotnet//blog">blog</a></li>
  <li><a href="https://twitter.com/xor" target="_blank">twitter</a></li>
  <li><a href="https://github.com/thisisparker" target="_blank">github</a></li>
</ul>


        <h1 class="masthead-title">
          <a href="/dotnet//" title="Home">parker higgins dot net</a>
          <!-- <small></small> -->
        </h1>
      </header>

      <main>
        <div class="posts">
  
  <article class="post">
    <h2 class="post-title">
      <a href="/dotnet/2017/09/which-states-hated-wesley/">
        Which states hated Wesley?
      </a>
    </h2>

    <time datetime="2017-09-08T15:20:59-04:00" class="post-date">08 Sep 2017</time>

    <p>One of my goals while at Recurse Center has been to improve my ability to manipulate and visualize data sets. To that end, I’ve been toying around with the <a href="https://www.ssa.gov/oact/babynames/limits.html">Social Security Administration’s baby name dataset</a>, which records the number of babies born with each given name every year, both federally and at the state level. Because I’ve also been watching <em>Star Trek: The Next Generation</em> along with the <a href="http://foreverdogproductions.com/fdpn/podcasts/treks-and-the-city/">Treks And The City podcast</a>, I chose to dig into information about the name “Wesley.”</p>

<p>On my first pass through the data I noticed that the name’s popularity dramatically spiked around 1976, and then tapered off for a few decades after. Honestly, that spike is the most interesting property of the whole graph, and I can’t explain it very well. But a funny secondary effect is that neither <em>TNG</em>‘s premiere nor the release of <em>The Princess Bride</em>—both in 1987—could prop up the name as it declined in popularity. The effect makes it look like it’s tumbling off a cliff, instead of regressing to the mean. This graph, including the label, was generated in Python’s <code class="language-plaintext highlighter-rouge">matplotlib</code>.</p>

<p><img src="https://parkerhiggins.net/wp-content/uploads/2017/09/figure_1-2.png" alt="" /></p>

<p>After looking at the federal data, I decided to dig into the state-level stuff, to give me a (long-anticipated!) opportunity to generate a choropleth map. Again, I cleaned up the data in Python, and then generated a map using a Javascript library called <code class="language-plaintext highlighter-rouge">&lt;a href="https://d3-geomap.github.io/"&gt;d3-geomap&lt;/a&gt;</code>. For a long time I’ve wanted to get more familiar with its parent library, d3, and this has been a nice opportunity to dip my toe into that.</p>

<p><img src="https://parkerhiggins.net/wp-content/uploads/2017/09/wesmap.png" alt="" /></p>

  </article>
  
  <article class="post">
    <h2 class="post-title">
      <a href="/dotnet/2017/08/new-bot-78_sampler-serving-up-old-records/">
        New bot: @78_sampler, serving up old records
      </a>
    </h2>

    <time datetime="2017-08-11T13:36:49-04:00" class="post-date">11 Aug 2017</time>

    <p>The Internet Archive hosts an <a href="https://archive.org/details/georgeblood">incredible collection</a> of over 25,000 <a href="http://great78.archive.org/">professionally digitized 78rpm records</a>. The great thing about a catalog that large is that, if you know what you want, you’re likely to find it. On the other hand, if you just want to browse it can be overwhelming and even intimidating. Each item could possibly be a delight, but it’s difficult to even think about individual records in the face of such a huge archive.</p>

<p>In that sense, would-be browsers face similar challenges with the Great 78 Project as they do with the <a href="https://usdawatercolors.nal.usda.gov/pom/home.xhtml">Pomological Watercolor Collection</a>—an archive <a href="https://parkerhiggins.net/tag/pomological-watercolors/">I’ve worked with a lot</a>. Sensing that similarity, I decided to build a tool like <a href="https://twitter.com/pomological">@pomological</a> to help surface individual records.</p>

<p><span class="citation"><a href="https://twitter.com/78_sampler">@78_sampler</a> tweets every two hours with a randomly selected record from the Archive’s collection. It was important to me that the audio fit smoothly and natively into a Twitter timeline, so I decided to render each tune into a video file using the Archive’s still image of the record as the visual. Twitter limits videos to 2:20—exactly 140 seconds, cute—which is shorter than most 78 tunes, so while rendering the video I truncate the clip at that point with a short audio fade at the end.</span></p>

<p>The <a href="https://github.com/thisisparker/78_sampler">code to do all this</a> is a short Python script which I’ve posted online. It relies on ffmpeg to do the video encoding. Crafting ffmpeg commands is famously convoluted, and it’s a little frustrating to format those commands to be called from Python. Maybe that’s something I’ll do differently in the future but, for now, this works and I can dip my cup into the deep Archive well with a little more ease than before.</p>

  </article>
  
  <article class="post">
    <h2 class="post-title">
      <a href="/dotnet/2017/07/hosting-change/">
        Hosting change
      </a>
    </h2>

    <time datetime="2017-07-28T17:54:36-04:00" class="post-date">28 Jul 2017</time>

    <p>Just a quick meta note: I’ve moved this site to a new hosting situation, but there shouldn’t be any disruption to its availability. I’ll probably also be looking into different CMS options while I’m here at Recurse Center.</p>

  </article>
  
  <article class="post">
    <h2 class="post-title">
      <a href="/dotnet/2017/07/he-did-the-monster-mosh-automated-datamoshing-with-tweaked-gop-lengths/">
        He did the monster mosh: automated datamoshing with tweaked GOP lengths
      </a>
    </h2>

    <time datetime="2017-07-21T17:52:32-04:00" class="post-date">21 Jul 2017</time>

    <p>After I posted yesterday about my <a href="https://parkerhiggins.net/2017/07/making-mosh-ups-automated-datamoshing-from-multiple-video-sources/">automated datamoshing experiment</a>, I got a <a href="https://twitter.com/ryanfb/status/888159099413811202">nice message on Twitter from developer and botmaker Ryan Bauman</a> who was able to run my script on his own videos. We talked for a bit about how it could be improved, and I had a major realization that I had to test immediately.</p>

<p>In yesterday’s post I mentioned that the relative prevalence of P-Frames precluded my preferred effect of stills from one video and movement from another. Too much image data was being drawn by the P-Frames for it to really get unrecognizable. I didn’t and still don’t know how to reduce the number of P-Frames per se, but the big realization was I could change the ratio of I-Frames to P-Frames by ratcheting the <a href="https://en.wikipedia.org/wiki/Group_of_pictures">“Group Of Pictures” size</a> way down. That variable determines how often a video includes an I-Frame, and I could set it easily in ffmpeg while re-encoding the source videos.</p>

<p>A normal default GOP size might be 250 frames — which is to say, no more than 10 seconds of video go by without a full I-Frame render. Poking around, I tried changing that number to a few different values to see what works best. Experimentally, a GOP size maximum of 48 frames (once every two seconds) seems to do the trick for two video sources. An excerpt of that video looks like this:</p>

<iframe allowfullscreen="allowfullscreen" frameborder="0" height="315" loading="lazy" src="https://www.youtube-nocookie.com/embed/TLsT1H7HOs0?rel=0" width="560">  
</iframe>

<p>What are the constraints on the GOP size? Here, as in many other moments of this project, my considerations are very different from most people’s. In most cases, you want I-Frames to happen frequently enough that cutting and seeking through the video can be relatively precise, but rare enough that the filesize can stay low. Since every I-Frame has to contain a full frame of image data, they can be large.</p>

<p>Because I’m swapping I-Frames at the byte level, and truncating or padding each frame to fit, every I-Frame insertion is a potential source of trouble. You can see in the video above, my encoder struggles in certain places. And given that I’m doing substitution, the closer my GOP gets to 1, the more my mosh-up starts to just look like a slideshow.</p>

<p>For single source videos, where I-Frames are likely to be more similar to each other, I was able to use an even shorter GOP length. Here’s a datamoshed version of the Countdown video with a GOP length of 25 frames.</p>

<iframe allowfullscreen="allowfullscreen" frameborder="0" height="315" loading="lazy" src="https://www.youtube-nocookie.com/embed/u4YJU4mV-7M?rel=0" width="560"></iframe>

<p>So, in conclusion: messing with GOP sizes means a different number of I-Frames which means more opportunities for hijinks in mangling the videos.</p>

  </article>
  
  <article class="post">
    <h2 class="post-title">
      <a href="/dotnet/2017/07/making-mosh-ups-automated-datamoshing-from-multiple-video-sources/">
        Making mosh-ups: automated datamoshing from multiple video sources
      </a>
    </h2>

    <time datetime="2017-07-20T15:19:34-04:00" class="post-date">20 Jul 2017</time>

    <p>Datamoshing is a glitch art technique applied to videos to intentionally create “pixel bleeding” and other digital motion artifacts. It became popular several years ago when it was used in near-simultaneous music videos by <a href="https://www.youtube.com/watch?v=mvqakws0CeU">Chairlift</a> and <a href="https://www.youtube.com/watch?v=wMH0e8kIZtE">Kanye West</a>. In those cases, and in the tutorials and techniques documented since then, the glitches are typically introduced to a single edited video, and done manually in a visual editing program.</p>

<p>My goal for <a href="https://github.com/thisisparker/automosh/blob/master/automosh.py">this project</a> was to use two separate video sources — to make a “mosh-up,” har har — and to completely automate the merger. The holy grail would be to use all the motion from one video over all the stills of another, to make sort of an animated <a href="https://en.wikipedia.org/wiki/Magic_Eye">Magic Eye</a> effect, but without the eye focus requirement. (Side note: it’s <a href="https://sploid.gizmodo.com/this-amazing-magic-eye-music-video-hides-fun-secret-mov-1513555083">possible and awesome</a> to actually create animated Magic Eyes, but that’s beside the point.)</p>

<p>As you’ll see, I fell a little short of that stretch goal, but still managed to make something that looks pretty cool.</p>

<h2 id="moshed-up-vids">Moshed-up vids</h2>

<p><a href="https://github.com/thisisparker/automosh/blob/master/automosh.py">The script I wrote</a> can create two kinds of datamoshes. The first takes a single video as a source and rearranges some key frames to glitch it out. Here’s an example of one such video, glitching up Beyoncé’s incredible <a href="https://www.youtube.com/watch?v=2XY3AvVgDns">Countdown video</a> with itself. I’ve muted the audio in this upload, but as output from the script it still sounds pretty good.</p>

<iframe allowfullscreen="allowfullscreen" frameborder="0" height="315" loading="lazy" src="https://www.youtube-nocookie.com/embed/a9iNDj0iaug?rel=0" width="560"></iframe>

<p>The second (and more exciting) kind of data moshes takes a certain kind of key frame from one video and replaces the same kind of frame in another video. All of the motion and some of the “re-drawings” of subsequent frames are pulled out of context, creating an effect that is a little surreal and unsettling. It’s not as precise as the pros do with their manual edits, but it also can automatically combine two sources in a way I’ve never seen before. (Here, I used Countdown again, and moshed it together with <a href="https://www.youtube.com/watch?v=pZ12_E5R3qc">Formation</a>.)</p>

<iframe allowfullscreen="allowfullscreen" frameborder="0" height="315" loading="lazy" src="https://www.youtube-nocookie.com/embed/SARwUp1P7oQ?rel=0" width="560"></iframe>

<p>Again, I’ve muted the audio, but in this case it would normally play back Partition without any noticeable <a href="https://www.youtube.com/watch?v=56qgO0C82vY">flaws</a>.</p>

<h2 id="how-it-works">How it works</h2>

<p>This moshing technique relies on some facts about how the H.264 spec compresses and stores its data. It really is a remarkable standard, and if you’re not familiar it’s absolutely worth reading the tribute that is <a href="https://sidbala.com/h-264-is-magic/">“H.264 is Magic”</a>. The gist is that only a very small portion of frames, dubbed I-Frames, contain all the image data necessary to draw a full screen. The other kinds of frames, P- and B-Frames, have partial screens and “motion” data.</p>

<p>Other popular datamoshing techniques include removing I-Frames altogether or duplicating P-Frames so the same motion is re-applied to an image. In this example, instead, I’m replacing I-Frames with other I-Frames, and I’m doing it simply by copying and pasting the bytes from one into the place of another, truncating or padding out the data so it fits exactly.</p>

<p>The reason I can’t get only the motion from one video and only the “textures” from another is that the P-Frames blend those two types of data together into a single frame. If I could figure out some way to isolate the image data in the frame, or to re-encode a video to consist entirely of I- and B-Frames, I could probably get a wilder effect.</p>

<p>As it stands, the output of this script is a video that <em>plays</em>, but is pretty badly mangled. If you try to play it back in a client that shows you errors, you’ll see a lot of complaints. For compatibility’s sake, I’ve manually transcoded these videos into another format and back.</p>

  </article>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/dotnet/blog/page4/">Older</a>
  
  
    <a class="pagination-item newer" href="/blog/page2/">Newer</a>
  
</div>

      </main>

      <footer class="footer">
        <small>
          &copy; <time datetime="2022-09-18T17:33:55-04:00">2022</time>. All rights reserved.
        </small>
      </footer>
    </div>

    
  </body>
</html>
